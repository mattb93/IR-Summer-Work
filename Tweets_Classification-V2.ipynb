{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs, re, json, os, time\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.mllib.fpm import FPGrowth\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, IDF, StopWordsRemover\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Create Spark and SQL context:</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"Text Classifier\")\n",
    "# if not sc:\n",
    "# sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Load Configuration File</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_config(config_file):\n",
    "    \"\"\"\n",
    "    Load collection configuration file.\n",
    "    \"\"\"\n",
    "    with open(config_file) as data_file:    \n",
    "        config_data = json.load(data_file)\n",
    "    return config_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Parse Tweets to tweet_id and tweet_text</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_tweet(line):\n",
    "    \"\"\"\n",
    "    Parses a tweet record having the following format collectionId-tweetId<\\t>tweetString\n",
    "    \"\"\"\n",
    "    fields = line.strip().split(\"\\t\")\n",
    "    if len(fields) == 2:\n",
    "        # The following regex just strips of an URL (not just http), any punctuations, \n",
    "        # or Any non alphanumeric characters\n",
    "        # http://goo.gl/J8ZxDT\n",
    "        text = re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",fields[1]).strip()\n",
    "        # remove terms <= 2 characters\n",
    "        text = ' '.join(filter(lambda x: len(x) > 2, text.split(\" \")))\n",
    "        # return tuple of (collectionId-tweetId, text)\n",
    "        return (fields[0], text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Load tweets from file into DataFrame:</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Load_tweets(collection_id):\n",
    "    tweets_file = os.path.join(data_dir , \"z_\" + collection_id)\n",
    "    print(\"Loading \" + tweets_file) \n",
    "    if not os.path.isdir(tweets_file):\n",
    "        print(tweets_file + \" folder doesn't exist.\")\n",
    "        return False\n",
    "    tweets = sc.textFile(tweets_file) \\\n",
    "              .map(parse_tweet) \\\n",
    "              .filter(lambda x: x is not None) \\\n",
    "              .map(lambda x: Row(id=x[0], text=x[1])) \\\n",
    "              .toDF() \\\n",
    "              .cache()\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<h1>Tokenize and remove stop words from tweet text:</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess_tweets(tweets):\n",
    "    tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "    tweets = tokenizer.transform(tweets)\n",
    "    remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "    tweets = remover.transform(tweets)\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Save unique tokens:</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Frequent pattern mining expect each row to have a unique set of tokens\n",
    "def save_unique_token(tweets):\n",
    "    tweets = (tweets\n",
    "      .rdd\n",
    "      .map(lambda x : (x.id, x.text, list(set(filter(None, x.filtered)))))\n",
    "      .toDF()\n",
    "      .withColumnRenamed(\"_1\",\"id\")\n",
    "      .withColumnRenamed(\"_2\",\"text\")\n",
    "      .withColumnRenamed(\"_3\",\"filtered\")).cache()\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Run Frequent Pattern Mining algorithm and save to output file:</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_FPM(tweets, collection):\n",
    "    model = FPGrowth.train(tweets.select(\"filtered\").rdd.map(lambda x: x[0]), minSupport=0.02)\n",
    "    result = sorted(model.freqItemsets().collect(), reverse=True)\n",
    "    # sort the result in reverse order\n",
    "    sorted_result = sorted(result, key=lambda item: int(item.freq), reverse=True)\n",
    "\n",
    "    # save output to file\n",
    "    with codecs.open(FP_dir + time.strftime(\"%Y%m%d-%H%M%S\") + '_'\n",
    "                            + collection[\"Id\"] + '_' \n",
    "                            + collection[\"name\"] + '.txt', 'w',encoding='utf-8') as file:\n",
    "        for item in sorted_result:\n",
    "            file.write(\"%s %s\\n\" % (item.freq, ' '.join(item.items)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Global Variables</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_dir = \"/home/hshahin/Spring2016_IR_Project/data/\"\n",
    "data_dir = os.path.join(base_dir , \"small_data\")\n",
    "models_dir = os.path.join(data_dir, \"models\")\n",
    "predictions_dir = os.path.join(data_dir, \"predictions\")\n",
    "FP_dir = base_dir + \"FPGrowth/\"\n",
    "config_file = \"collections_config.json\"\n",
    "config_data = load_config(os.path.join(base_dir , config_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Phase I: Run FPM to all data sets</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /home/hshahin/Spring2016_IR_Project/data/small_data/z_602\n",
      "Loading /home/hshahin/Spring2016_IR_Project/data/small_data/z_541\n",
      "Loading /home/hshahin/Spring2016_IR_Project/data/small_data/z_668\n",
      "Loading /home/hshahin/Spring2016_IR_Project/data/small_data/z_700\n",
      "Loading /home/hshahin/Spring2016_IR_Project/data/small_data/z_686\n",
      "Loading /home/hshahin/Spring2016_IR_Project/data/small_data/z_694\n",
      "Loading /home/hshahin/Spring2016_IR_Project/data/small_data/z_532\n",
      "/home/hshahin/Spring2016_IR_Project/data/small_data/z_532 folder doesn't exist.\n"
     ]
    }
   ],
   "source": [
    "for x in config_data[\"collections\"]:\n",
    "    tweets = Load_tweets(x[\"Id\"])\n",
    "    if tweets:\n",
    "        tweets = preprocess_tweets(tweets)\n",
    "        tweets = save_unique_token(tweets)\n",
    "        run_FPM(tweets, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Manually choose frequent patterns and write them in the configuration file.</h1>\n",
    "<h1>Reload the configuration file:</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get FP from config file\n",
    "config_data = load_config(os.path.join(base_dir , config_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Create training data of positive and negative samples as DataFrame</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_training_data(tweets, freq_patterns):\n",
    "    # Tweets contains the frequent pattern terms will be considered as positive samples\n",
    "    positive_tweets = (tweets\n",
    "      .rdd\n",
    "      .filter(lambda x: set(freq_patterns).issubset(x.filtered))\n",
    "      .map(lambda x : (x[0], x[1], x[2], 1.0))\n",
    "      .toDF()\n",
    "      .withColumnRenamed(\"_1\",\"id\")\n",
    "      .withColumnRenamed(\"_2\",\"text\")\n",
    "      .withColumnRenamed(\"_3\",\"filtered\")\n",
    "      .withColumnRenamed(\"_4\",\"label\"))\n",
    "\n",
    "    # calculate a fraction of positive samples to extract equivalent number of negative samples\n",
    "    positive_fraction = float(positive_tweets.count()) / tweets.count()\n",
    "\n",
    "    # Negative samples will be randomly selected from non_positive samples\n",
    "    negative_tweets = (tweets\n",
    "      .rdd\n",
    "      .filter(lambda x: not set(freq_patterns).issubset(x[2]))                   \n",
    "      .sample(False, positive_fraction, 12345)\n",
    "      .map(lambda x : (x[0], x[1], x[2], 0.0))\n",
    "      .toDF()\n",
    "      .withColumnRenamed(\"_1\",\"id\")\n",
    "      .withColumnRenamed(\"_2\",\"text\")\n",
    "      .withColumnRenamed(\"_3\",\"filtered\")\n",
    "      .withColumnRenamed(\"_4\",\"label\"))\n",
    "    training_data = positive_tweets.unionAll(negative_tweets)\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Train LogisticRegression Classifier:</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_lg(training_data, collection):\n",
    "    # Configure an ML pipeline, which consists of the following stages: hashingTF, idf, and lr.\n",
    "    hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"TF_features\")\n",
    "    idf = IDF(inputCol=hashingTF.getOutputCol(), outputCol=\"features\")\n",
    "    pipeline1 = Pipeline(stages=[hashingTF, idf])\n",
    "\n",
    "    # Fit the pipeline1 to training documents.\n",
    "    model1 = pipeline1.fit(training_data)\n",
    "\n",
    "    lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "    pipeline2 = Pipeline(stages=[model1, lr])\n",
    "\n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(hashingTF.numFeatures, [10, 100, 1000, 10000]) \\\n",
    "        .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "        .build()\n",
    "\n",
    "    crossval = CrossValidator(estimator=pipeline2,\n",
    "                              estimatorParamMaps=paramGrid,\n",
    "                              evaluator=BinaryClassificationEvaluator(),\n",
    "                              numFolds=5)  \n",
    "\n",
    "    # Run cross-validation, and choose the best set of parameters.\n",
    "    cvModel = crossval.fit(training_data)\n",
    "\n",
    "#     model_path = os.path.join(models_dir , time.strftime(\"%Y%m%d-%H%M%S\") + '_'\n",
    "#                             + collection[\"Id\"] + '_' \n",
    "#                             + collection[\"name\"])\n",
    "#     cvModel.save(sc, model_path)\n",
    "    return cvModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Evaluating LogisticRegression model on training data:</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_training_score_lg(lg_model, training_data):\n",
    "    training_prediction = lg_model.transform(training_data)\n",
    "    selected = training_prediction.select(\"label\", \"prediction\").rdd.map(lambda x: (x[0], x[1]))\n",
    "    training_error = selected.filter(lambda (label, prediction): label != prediction).count() / float(tweets.count())\n",
    "    print(\"Training Error = \" + str(training_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Prepare testing data:</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_testing_data(tweets):\n",
    "    testing_data = (tweets\n",
    "                    .rdd\n",
    "                    .map(lambda x: Row(id=x[0], filtered=x[2]))\n",
    "                    .toDF())\n",
    "    return testing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Prediction Fucntion</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lg_prediction(lg_model, testing_data, collection):\n",
    "    # Perfom predictions on test documents and save columns of interest to a file.\n",
    "    prediction = lg_model.transform(testing_data)\n",
    "    selected = prediction.select(\"id\", \"prediction\", \"probability\")\n",
    "    prediction_path = os.path.join(predictions_dir , time.strftime(\"%Y%m%d-%H%M%S\") + '_'\n",
    "                            + collection[\"Id\"] + '_' \n",
    "                            + collection[\"name\"])\n",
    "    print(prediction_path)\n",
    "    def saveData(data):\n",
    "        with open(prediction_path, 'a') as f:\n",
    "            f.write(data.id+\"\\t\"+str(data.probability[1])+\"\\n\")\n",
    "    selected.foreach(saveData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Phase II: Train classifier and perform prediction</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /home/hshahin/Spring2016_IR_Project/data/small_data/z_602\n",
      "Training Error = 9.71985225825e-05\n",
      "/home/hshahin/Spring2016_IR_Project/data/small_data/predictions/20160413-200131_602_Germanwings\n",
      "Loading /home/hshahin/Spring2016_IR_Project/data/small_data/z_541\n",
      "Training Error = 0.0\n",
      "/home/hshahin/Spring2016_IR_Project/data/small_data/predictions/20160413-200305_541_NAACPBombing\n",
      "Loading /home/hshahin/Spring2016_IR_Project/data/small_data/z_668\n",
      "Training Error = 6.73038093956e-05\n",
      "/home/hshahin/Spring2016_IR_Project/data/small_data/predictions/20160413-200402_668_houstonflood\n",
      "Loading /home/hshahin/Spring2016_IR_Project/data/small_data/z_700\n",
      "Training Error = 0.00106288751107\n",
      "/home/hshahin/Spring2016_IR_Project/data/small_data/predictions/20160413-200449_700_wdbj7 shooting \n",
      "Loading /home/hshahin/Spring2016_IR_Project/data/small_data/z_686\n",
      "Training Error = 0.00031944557243\n",
      "/home/hshahin/Spring2016_IR_Project/data/small_data/predictions/20160413-201031_686_Obamacare"
     ]
    }
   ],
   "source": [
    "for x in config_data[\"collections\"]:\n",
    "    tweets = Load_tweets(x[\"Id\"])\n",
    "    if tweets:\n",
    "        freq_patterns = x[\"FP\"]\n",
    "        tweets = preprocess_tweets(tweets)\n",
    "        training_data = create_training_data(tweets, freq_patterns)\n",
    "        lg_model = train_lg(training_data, x)\n",
    "        get_training_score_lg(lg_model, training_data)\n",
    "        testing_data = create_testing_data(tweets)\n",
    "        lg_prediction(lg_model, testing_data, x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
